package game.train;

/**
 * Launch the Simplified DQN Live Training!
 * 
 * This provides immediate visual AI learning without complex integration.
 * Perfect for watching neural networks learn decision-making in real-time.
 * 
 * Usage:
 *   java -cp out game.train.SimpleDQNMain
 * 
 * What you'll see:
 *   - Green dot (AI player) learning to avoid red circles (asteroids)
 *   - Real-time learning statistics showing improvement
 *   - Epsilon-greedy exploration gradually becoming more strategic
 *   - Learning curve showing AI getting better over time
 */
public class SimpleDQNMain {
    public static void main(String[] args) {
        System.out.println("================================================");
        System.out.println("    ü§ñ DEEP Q-LEARNING LIVE TRAINING DEMO ü§ñ   ");
        System.out.println("================================================");
        System.out.println();
        System.out.println("üéØ EDUCATIONAL NEURAL NETWORK ARCHITECTURE:");
        System.out.println("   Perceptron ‚Üí Layer ‚Üí Network ‚Üí Agent");
        System.out.println("   Individual neurons ‚Üí Parallel processing ‚Üí Complete network ‚Üí Learning behavior");
        System.out.println();
        System.out.println("üß† LEARNING PROCESS:");
        System.out.println("   ‚Ä¢ Episodes 1-10:   Random chaotic movement");
        System.out.println("   ‚Ä¢ Episodes 20-50:  Basic pattern recognition");
        System.out.println("   ‚Ä¢ Episodes 100+:   Strategic avoidance behavior");
        System.out.println("   ‚Ä¢ Episodes 500+:   Sophisticated decision making");
        System.out.println();
        System.out.println("üìä LIVE STATISTICS:");
        System.out.println("   ‚Ä¢ Real-time learning curves");
        System.out.println("   ‚Ä¢ Exploration rate (epsilon) decay");  
        System.out.println("   ‚Ä¢ Episode rewards and progress");
        System.out.println("   ‚Ä¢ Experience replay buffer status");
        System.out.println();
        System.out.println("üéÆ CONTROLS:");
        System.out.println("   ‚Ä¢ Pause/Resume - Toggle training");
        System.out.println("   ‚Ä¢ Reset Agent - Start learning from scratch");
        System.out.println("   ‚Ä¢ Speed Controls - 1x, 2x, 5x, 10x faster training");
        System.out.println("   ‚Ä¢ Turbo Mode - Maximum speed (100x) with frame skipping");
        System.out.println();
        System.out.println("üöÄ Starting live training interface...");
        System.out.println();
        
        try {
            // Create and show the training window
            SimpleDQNTrainer trainer = new SimpleDQNTrainer();
            trainer.setVisible(true);
            
            System.out.println("‚úÖ Training interface launched successfully!");
            System.out.println();
            System.out.println("üéØ WATCH THE AI LEARN:");
            System.out.println("   Green dot = AI player (learning to survive)");
            System.out.println("   Red circles = Asteroids (obstacles to avoid)");
            System.out.println("   Statistics panel = Real-time learning progress");
            System.out.println();
            System.out.println("üß† NEURAL NETWORK LEARNING:");
            System.out.println("   The AI uses backpropagation to learn from each decision.");
            System.out.println("   You'll see immediate improvement as it learns to avoid collisions!");
            System.out.println();
            System.out.println("üìà EXPECTED PROGRESSION:");
            System.out.println("   Initial: Random movement, frequent collisions");
            System.out.println("   Learning: Gradual collision avoidance");
            System.out.println("   Mastered: Strategic movement and survival");
            
        } catch (Exception e) {
            System.err.println("‚ùå Error starting DQN training: " + e.getMessage());
            e.printStackTrace();
        }
    }
}
